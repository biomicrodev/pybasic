#!/usr/bin/env python3

import argparse
from pathlib import Path
from typing import List, Set, Optional, Tuple, Union

import PIL.Image
import dask
import dask.array as da
import numpy as np
import numpy.typing as npt
import skimage.transform
import tifffile
from skimage.util import img_as_float
from tifffile import tifffile

from pybasic._bin import _validate_iter_dims, _flatten_paths, _query_yes_no
from pybasic.basic import basic
from pybasic.utils import timed_ctx

ShapeLike = Union[List[int], Tuple[int, ...]]


def resize(im: npt.NDArray, shape: ShapeLike) -> npt.NDArray:
    return skimage.transform.resize(im, output_shape=shape, order=1, mode="symmetric")


def read_images(
    paths: List[Path], *, working_size: int, iter_dims: Optional[Set[int]] = None
) -> da.Array:
    """
    It's possible that the error messages on a raised exception might not make sense.

    TODO: Check what kinds of error messages are raised on invalid inputs, e.g. unequal
    array shapes.
    """
    if iter_dims is None:
        iter_dims = {}

    im1 = tifffile.imread(paths[0])

    # we resize the image axes to the working size
    shape = [s if i in iter_dims else working_size for i, s in enumerate(im1.shape)]

    # load, transform, reshape, and stack using dask
    images = [dask.delayed(tifffile.imread)(path) for path in paths]
    images = [dask.delayed(img_as_float)(im) for im in images]
    images = [
        dask.delayed(lambda im: resize(im, shape), name=resize.__name__)(im)
        for im in images
    ]
    images = [da.from_delayed(i, shape=shape, dtype=float) for i in images]
    images = da.stack(images)
    return images


def parse_args() -> argparse.Namespace:
    filename = Path(__file__).name

    parser = argparse.ArgumentParser(
        description=f"{filename} - Generate background and shading correction images for microscopy images"
    )
    parser.add_argument(
        "images",
        type=str,
        nargs="+",
        help="paths to images and/or folders containing images",
    )
    parser.add_argument(
        "--and-darkfield",
        dest="compute_darkfield",
        action="store_true",
        help="compute darkfield (in addition to flatfield)",
    )
    parser.add_argument(
        "--iter-dims",
        metavar="N",
        type=int,
        nargs="+",
        help="if multichannel images, must specify dims to iterate over (dims other than YX)",
    )
    parser.add_argument(
        "--rgb",
        action="store_true",
        help="shorthand for --iter-dims but in the special case of RGB images",
    )
    parser.add_argument(
        "--out",
        metavar="PATH",
        type=str,
        default=".",
        help="output folder; current folder if not specified",
    )
    # parser.add_argument(
    #     "--log",
    #     dest="log",
    #     metavar="PATH",
    #     type=str,
    #     help="log file for debugging; saved to output location",
    # )
    parser.add_argument(
        "--flatfield-reg",
        metavar="VALUE",
        dest="flatfield_reg",
        type=float,
        help="flatfield regularization parameter",
    )
    parser.add_argument(
        "--darkfield-reg",
        metavar="VALUE",
        dest="darkfield_reg",
        type=float,
        help="darkfield regularization parameter",
    )
    parser.add_argument(
        "--working-size",
        metavar="VALUE",
        type=int,
        default=128,
        help="resize both image dims to this",
    )
    parser.add_argument(
        "--max-images",
        metavar="VALUE",
        type=int,
        default=500,
        help="maximum number of images to load",
    )
    return parser.parse_args()


def _is_image(p: Path) -> bool:
    try:
        fp = PIL.Image.open(p)
    except PIL.UnidentifiedImageError:
        is_image = False
    else:
        fp.close()
        is_image = True

    return is_image


def _check_output_files(out: Path, compute_darkfield: bool):
    flat_exists = (out / "flatfield.tiff").exists()
    dark_exists = compute_darkfield and (out / "darkfield.tiff").exists()

    msg = None
    if flat_exists and dark_exists:
        msg = "Flatfield and darkfield images already exist. Overwrite?"
    elif flat_exists and not dark_exists:
        msg = "Flatfield image already exists. Overwrite?"

    if msg is None:
        return
    overwrite = _query_yes_no(msg, default="no")
    if overwrite:
        return
    else:
        raise RuntimeError("File already exists; quitting")


def run():
    args = parse_args()

    # check if destination files already exist before doing anything
    out = Path(args.out)
    try:
        _check_output_files(out, args.compute_darkfield)
    except RuntimeError:
        return

    # normalize paths; sort, then take first max_images images, for reproducibility
    # purposes
    paths = [Path(p) for p in args.images]
    paths = list(set(paths))  # make unique
    paths = sorted(list(_flatten_paths(paths, depth=1)))
    paths = paths[: args.max_images]

    # check if image by trying to open it with pillow; could be slow?
    # is it performant to do this rather than have a dask worker try and fail?
    # we can check the file extension, but those can be unreliable
    paths = [p for p in paths if _is_image(p)]
    if len(paths) == 0:
        raise ValueError("No images provided/found")
    elif len(paths) == 1:
        raise ValueError("One image is not sufficient for illumination correction")

    # get first image
    orig_im_shape = tifffile.imread(paths[0]).shape

    # reconcile first image with provided arguments
    # for now, rgb is interpreted as just a special case of multichannel
    iter_dims = set(args.iter_dims) if args.iter_dims is not None else set()
    iter_dims = _validate_iter_dims(orig_im_shape, rgb=args.rgb, iter_dims=iter_dims)

    # read_images
    stack = read_images(
        paths,
        working_size=args.working_size,
        iter_dims=iter_dims,
    )
    print(f"Image stack with shape {stack.shape}")

    if len(iter_dims) == 0:
        # nothing to iterate over, so we pass entire array to basic
        stack = np.asarray(stack)
        assert stack.ndim == 3
        flatfield, darkfield = basic(
            stack,
            flatfield_reg=args.flatfield_reg,
            darkfield_reg=args.darkfield_reg,
            compute_darkfield=args.compute_darkfield,
        )

    else:
        """
        This is a little awkward with dimension wrangling because I would like to keep
        the dimension order of the original images the same, and I need to support an
        arbitrary set of iter dims (as far as I'm concerned). We could probably make
        the code more legible just by moving dims around before basic, and move them
        back afterwards.
        """

        def func(a: npt.NDArray) -> Tuple[npt.NDArray, npt.NDArray]:
            if a.size == 1:
                # dask (or gufunc) runs this function at least once with an array of
                # size 1, just to check what the output is. because this function is
                # computationally expensive, we provide dummy arrays with the same dtype
                return (
                    np.zeros(shape=(), dtype=a.dtype),
                    np.zeros(shape=(), dtype=a.dtype),
                )

            # remove, but keep track of, singular dimensions
            sing_dims = [i for i in range(a.ndim) if a.shape[i] == 1]
            a = a.squeeze()

            flatfield, darkfield = basic(
                a,
                flatfield_reg=args.flatfield_reg,
                darkfield_reg=args.darkfield_reg,
                compute_darkfield=args.compute_darkfield,
            )

            # recover original shape
            flatfield = np.expand_dims(flatfield, sing_dims)
            darkfield = np.expand_dims(darkfield, sing_dims)

            return flatfield, darkfield

        # append one to the iter dims
        # TODO: is it worth using dstack vs stack to avoid shifting between image dims
        # and stack dims?
        stack_iter_dims = [ax + 1 for ax in sorted(list(iter_dims))]

        # rechunk, because gufunc applies func one chunk at a time
        chunksize = [None if i not in stack_iter_dims else 1 for i in range(stack.ndim)]
        stack = stack.rechunk(chunksize)

        # specify input and output dims to gufunc
        # input_dims are relative to stack
        input_dims = tuple(i for i in range(stack.ndim) if i not in stack_iter_dims)
        assert len(input_dims) == 3
        # output_dims are relative to image, and since we lose the first dimension after
        # applying basic, we have to shift by one
        output_dims = tuple(
            i - 1 for i in range(1, stack.ndim) if i not in stack_iter_dims
        )
        assert len(output_dims) == 2

        res = da.apply_gufunc(
            func,
            "(c,i,j) -> (i,j), (i,j)",
            stack,
            axes=[input_dims, output_dims, output_dims],
            allow_rechunk=True,
        )
        flatfield, darkfield = dask.compute(*res)

    # resize back to original shape
    flatfield = resize(flatfield, orig_im_shape)
    if args.compute_darkfield:
        darkfield = resize(darkfield, orig_im_shape)

    out.mkdir(exist_ok=True, parents=True)
    with timed_ctx("Written", print):
        tifffile.imwrite(out / "flatfield.tiff", flatfield, compression="zlib")
        if args.compute_darkfield:
            tifffile.imwrite(out / "darkfield.tiff", darkfield, compression="zlib")


if __name__ == "__main__":
    run()
